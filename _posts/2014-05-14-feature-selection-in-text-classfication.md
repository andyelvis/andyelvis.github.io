---
layout: post
title: 文本分类的特征选择方法
categories:
- machine learning
tags:
- text categorization
---

## 1、文档频率DF（Document Frequency）

文档频率表示一个单词出现的文档数目。我们计算训练语料中的每一个单词的文档频率，对于文档频率小于预先定义阈值的单词，将其从特征空间中移除。基于的基本假设是很少出现的单词既对分类预测没有多大意义，也对全局性能没有什么影响。移除文档频率比较小的单词可以有效降低特征空间的维度，如果这些单词同时也是噪音单词的话，对分类准确性也有提升。

文档频率阈值化是降低词表维度最简单的方式，它可以以相对训练文档数目线性增长的计算复杂度，扩展到非常大训练语料。然而，通常将其看做提高效率的一种特定方式，而不是选取特征的原则标准。

---

## 2、信息增益IG（Information Gain）

在机器学习领域，信息增益常常被用作评估单词好坏的标准。通过知道一个单词在一篇文档中是否出现，衡量这个单词可以给分类预测带来多少信息量。假设$\\{c_i\\}^{m}_{i=1}$表示目标空间里类别的集合，那么单词t的信息增益可以表示如下：

$$
\begin{align*}
G(t)=&-\sum^{m}_{i=1}P_r(c_i)logP_r(c_i)\\
+&P_r(t)\sum^{m}_{i=1}P_r(c_i|t)logP_r(c_i|t)\\
+&P_r(\overline{t})\sum^{m}_{i=1}P_r(c_i|\overline{t})logP_r(c_i|\overline{t})
\end{align*}
$$

给定一个训练语料，计算每一个单词的信息增益，对于信息增益小于预先定义阈值的单词，将其从特征空间中移除。计算包括给定一个单词，一个类出现的条件概率的估计，以及信息熵的计算。

---

## 3、互信息MI（Mutual information）

互信息经常被用在词汇联想等统计语言模型相关的应用中。考虑训练语料中的一个单词t和一个类别c，A表示t和c共同出现的次数，B表示t出现c没有出现的次数，C表示c出现t没有出现的次数，N表示总文档数，那么t和c之间的互信息定义如下：

$$
I(t,c)=log\frac{P_r(t∧c)}{P_r(t)×P_r(c)}
$$

估计公式如下：

$$
I(t,c)≈log\frac{A×N}{(A+C)×(A+B)}
$$

如果t和c是相互独立的，那么$I(t,c)$就会有一个零自然价值问题。为了度量一个单词在全局特征选择中的好坏，我们用如下两种方式组合单词在不同类别下的分数：

$$
I_{avg}(t)=\sum^{m}_{i=1}P_r(c_i)I(t,c_i)
$$

$$
I_{max}(t)={max}^{m}_{i=1}\{I(t,c_i)\}
$$

互信息的一个缺点是分数会被单词的边缘概率严重影响，从如下等价的形式看：

$$
I(t,c)=logP_r(t|c)-logP_r(t)
$$

对于那些有着相等条件概率$P_r(t|c)$的单词，很少有单词会比常见单词的分数更高。
因此，在有不同频率的单词之间，分数不具有可比性。

---

## 4、${\chi}^2$统计（CHI）

${\chi}^2$统计度量单词t和类别c之间是否缺乏独立性，可以用自由度与${\chi}^2$分布比较来判断偏激性。假设A表示t和c共同出现的次数，B表示t出现c没有出现的次数，C表示c出现t没有出现的次数，D表示c和t都没有出现的次数，N表示总文档数，那么单词好坏的度量可以定义如下：

$$
{\chi}^2(t,c)=\frac{N\times(AD-CB)^2}{(A+C)\times(B+D)\times(A+B)\times(C+D)}
$$

如果t和c是相互独立的，那么${\chi}^2$统计有个零自然值的问题。为每个类计算单词和类别之间的${\chi}^2$统计，然后将单词相对特定类别的分数合并成两个分数：

$$
{\chi}^2_{avg}(t)=\sum^{m}_{i=1}P_r(c_i){\chi}^2(t,c_i)
$$

$$
{\chi}^2_{max}(t)={max}^{m}_{i=1}\{ {\chi}^2(t,c_i) \}
$$

CHI和MI的最大区别是${\chi}^2$是一个归一化值，因此同一个类别下不同单词的${\chi}^2$值是可以比较的。但是对于低频词，这个归一化的特性会失去，所以${\chi}^2$统计对于低频词是不可靠的。

---

## 5、Term Strength (TS)

TS方法基于一个单词很可能出现在密切相关的文档中的理念来估计单词的重要性。TS的计算是一个条件概率估计，即如果有一对相关文档x和y，一个单词出现在x中，那么这个单词也出现在y中的条件概率：

$$
s(t)=P_r(t \in y|t \in x)
$$

TS基于文档聚类，假设相关文档会共用许多单词，那么出现在相关文档最重叠区域的单词是最有信息量的。TS计算所需的参数就是文档相似度阈值。两篇文档如何接近才被认为是一个相关对。这里使用AREL，在阈值调优的时候，每篇文档的平均相关文档数。我们计算训练数据集中所有文档的相似度得分，然后在文档对的相似度值上尝试不同的阈值，选择最合理的阈值。AERL的值不断的通过实验选择，看它的优化效果有多好。根据已有的一些研究结果，AERL的值在10到20之间会达到满意的效果。

---

## 参考资料

- [a comparative study on feature selection in text categorization](http://courses.ischool.berkeley.edu/i256/f06/papers/yang97comparative.pdf)